{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T04:27:08.275222Z",
     "start_time": "2024-06-10T04:27:08.262368Z"
    }
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "#load_dotenv()\n",
    "\n",
    "#OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") # (Optional), if OpenAI Model is used\n",
    "\n",
    "MODEL = \"mistral:latest\" # Name of the model used by Ollama\n",
    "\n",
    "COLLECTION_NAME = 'doc_qa_db' # Name of the Collection to be created\n",
    "\n",
    "DIMENSION = 768 # Dimension of the embeddings\n",
    "\n",
    "URI = 'http://localhost:19530' # Connection parameters for the Milvus Server"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T04:27:13.358384Z",
     "start_time": "2024-06-10T04:27:12.984682Z"
    }
   },
   "source": [
    "# Import LLM and Embeddings\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "model = Ollama(model=MODEL)\n",
    "# print(model.invoke(\"Who is the fastest football player in the world?\"))\n",
    "embeddings = OllamaEmbeddings(model=\"mistral:latest\")"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T04:27:33.932821Z",
     "start_time": "2024-06-10T04:27:16.975559Z"
    }
   },
   "source": [
    "# (Optional) When you use Open AI Model, you have to parse the output\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = model | parser\n",
    "chain.invoke(\"Tell me a joke\")"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "# (Example) Use a PDF and split it to later save it into the Vector Store and do Question Answering with it\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"Optimierung Cloud-zentrierter Anwendungslandschaften durch Application Portfolio Management.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "pages"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "# Configure the prompt template that is used to ask the LLM\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "print(prompt.format(context=\"Here is some context\", question=\"Here is a question\"))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "# Build a LangChain chain containing the prompt and the model\n",
    "\n",
    "chain = prompt | model \n",
    "chain.input_schema.schema()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "# Testing the chain\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"context\": \"My name is Marvin\",\n",
    "        \"question\": \"What is my girlfriends name?\"\n",
    "    }\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "# Use Milvus as Vectorstore\n",
    "\n",
    "from langchain_community.vectorstores import Milvus\n",
    "\n",
    "connection_args = {'uri': URI }\n",
    "\n",
    "vectorstore = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args=connection_args,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    drop_old=True\n",
    ").from_documents(\n",
    "    pages,\n",
    "    embedding=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_args=connection_args\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "# (Optional) When not using Milvus, this is a simple alternative for a basic vector store\n",
    "\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(\n",
    "    pages,\n",
    "    embedding=embeddings\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "source": [
    "# Do a similarity search for a query\n",
    "\n",
    "query = \"Software Lifecycle\"\n",
    "docs = vectorstore.similarity_search(query)\n",
    "\n",
    "print(len(docs))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "source": [
    "# Test by using the vectorstore as a retriever. The retriever gives back the relevant pages based on the query invoked\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "retriever.invoke(\"LeanIX\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "source": [
    "# Build a chain additionally context information from the Vector database\n",
    "\n",
    "from operator import itemgetter\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever, \n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | prompt \n",
    "    | model\n",
    ")\n",
    "\n",
    "print(chain.invoke({\"question\": \"Was ist LeanIX?\"}))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "source": [
    "# Alternative way to print the result so every character is printed single in a stream as in ChatGPT\n",
    "\n",
    "for s in chain.stream({\"question\": \"Was ist Cloud Computing?\"}):\n",
    "    print(s, end=\"\", flush=True)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
